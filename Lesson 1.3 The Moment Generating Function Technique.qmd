---
title: "Lesson 1.3"
subtitle: "The Moment Generating Function Technique"
format: pdf
editor: visual
font: 12px
---
\setcounter{page}{11}

## Introduction

The moment-generating function method for finding the probability distribution of a function of random variables $Y_1, Y_2, \ldots, Y_n$ is based on the following uniqueness theorem.

**Theorem** (Uniqueness Property of MGFs)

Let $m_X(t)$ and $m_Y(t)$ denote the moment-generating functions of random variables $X$ and $Y$, respectively. If both moment-generating functions exist and $m_X(t) = m_Y(t)$ for all values of $t$, then $X$ and $Y$ have the same probability distribution.\

*Remark*:

The MGF completely determines the distribution of a random variable. Once the moment-generating function for $U = g(Y)$ has been found, it is compared with the moment-generating function for random variables with well-known distributions.

## The Moment Generating Function Technique

The steps involve in using the MGF techniques are:

1.  Derive the MGF of $U = g(Y)$, which is defined as $m_U(t) = E[e^{tU}]$.

2.  Try to compare $m_U(t)$ with the MGF of existing probability distributions.

3.  Because of the Uniqueness Theorem, $U$ must have the distribution as the one whose MGF you have recognized in (2).

### Example 1.3.1

Suppose $Y \sim Gamma(\alpha, \beta)$. Derive the distribution of $U = g(Y) = 2Y/\beta$ using the MGF technique.

**SOLUTION**

Since $Y \sim Gamma(\alpha, \beta)$, then

$$
m_Y(t) = \left(\frac{1}{1-\beta t} \right)^{\alpha}
$$

Now,

$$
\begin{aligned}
m_U(t) &= E[e^{tU}] \\
&= E[e^{t(2Y/\beta)}] \\
&= E[e^{(2t/\beta)Y}] \\
&= E[e^{t'Y}],\,\text{where:}\, t' =\frac{2t}{\beta} \\
&= m_Y(t') \\
&= \left(\frac{1}{1-\beta t'} \right)^{\alpha} \\
&= \left(\frac{1}{1-\beta (\frac{2t}{\beta})} \right)^{\alpha} \\
&= \left(\frac{1}{1-2t} \right)^{\alpha}
\end{aligned}
$$

The resulting MGF is the MGF of a Chi-square distribution with degrees of freedom equal to $2\alpha$. Therefore, $U \sim \chi^2 (2\alpha)$.

\
*Remark*:

The MGF techniques i very useful when we have a independent random variables $Y_1, Y_2, \dots, Y_n$ and we wanted to derive the distribution of the sum $U = Y_1 + Y_2 + \cdots + Y_n$, for example.

$$
\begin{aligned}
m_U(t) &= E[e^{tU}] \\
&= E[e^{t(Y_1 + Y_2 + \cdots + Y_n)}] \\
&= E[e^{tY_1 + tY_2 + \cdots + tY_n}] \\
&= E[e^{tY_1} \times e^{tY_2} \times \cdots \times e^{tY_n}] \\
&= E[e^{tY_1}] \times E[e^{tY_2}] \times \cdots \times E[e^{tY_n}], \text{because of independence} \\
&= m_{Y_1}(t) \times m_{Y_2}(t) \times \cdots \times m_{Y_n}(t) \\
&= [m_Y(t)]^n,\, \text{if iid}
\end{aligned}
$$

### Example 1.3.2

Suppose we have an iid sample $Y_1, Y_2, \ldots, Y_n$ from a Bernoulli distribution. What is the distribution of the sum $U = Y_1 + Y_2 + \ldots + Y_n$?

**SOLUTION**

Recall that the MGF of Bernoulli distributed random variable $Y$ is given by

$$
m_Y(t) = q + pe^t, \, \text{where:}\, q = 1- p
$$

Using the above result, the MGF of $U$ is

$$
m_Y(t) = [m_Y(t)]^n = [q + pe^t]^n
$$

The resulting MGF resembles the MGF of a binomial distribution with parameter $p$. In other words, $U \sim Bi(n,p)$.

### Example 1.3.3

Suppose we have a random sample $Y_1, Y_2, \ldots, Y_n$ from an exponential distribution with mean $\beta$. What is the distribution of the sum $U = Y_1 + Y_2 + \ldots + Y_n$?

**SOLUTION** \[Left as a classroom exercise!\]
